[2025-06-05T17:33:58.880+0300] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-06-05T17:33:58.907+0300] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: push_autopark_import_pipeline.create_base_tables manual__2025-06-05T14:33:48.957618+00:00 [queued]>
[2025-06-05T17:33:58.916+0300] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: push_autopark_import_pipeline.create_base_tables manual__2025-06-05T14:33:48.957618+00:00 [queued]>
[2025-06-05T17:33:58.917+0300] {taskinstance.py:2866} INFO - Starting attempt 1 of 1
[2025-06-05T17:33:58.934+0300] {taskinstance.py:2889} INFO - Executing <Task(SQLExecuteQueryOperator): create_base_tables> on 2025-06-05 14:33:48.957618+00:00
[2025-06-05T17:33:58.938+0300] {standard_task_runner.py:72} INFO - Started process 20065 to run task
[2025-06-05T17:33:58.946+0300] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'push_autopark_import_pipeline', 'create_base_tables', 'manual__2025-06-05T14:33:48.957618+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/push_autopark_import_pipeline.py', '--cfg-path', '/var/folders/p0/qnxbkr491pd245gy9gbyyml40000gn/T/tmp32373w76']
[2025-06-05T17:33:58.950+0300] {standard_task_runner.py:105} INFO - Job 7: Subtask create_base_tables
[2025-06-05T17:33:59.026+0300] {task_command.py:467} INFO - Running <TaskInstance: push_autopark_import_pipeline.create_base_tables manual__2025-06-05T14:33:48.957618+00:00 [running]> on host MacBook-Pro-Margarita.local
[2025-06-05T17:33:59.154+0300] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='push_autopark_import_pipeline' AIRFLOW_CTX_TASK_ID='create_base_tables' AIRFLOW_CTX_EXECUTION_DATE='2025-06-05T14:33:48.957618+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-06-05T14:33:48.957618+00:00'
[2025-06-05T17:33:59.156+0300] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-06-05T17:33:59.157+0300] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-06-05T17:33:59.158+0300] {logging_mixin.py:190} INFO - Current task name:create_base_tables state:running start_date:2025-06-05 14:33:58.907817+00:00
[2025-06-05T17:33:59.158+0300] {logging_mixin.py:190} INFO - Dag name:push_autopark_import_pipeline and current dag run status:running
[2025-06-05T17:33:59.159+0300] {taskinstance.py:731} INFO - ::endgroup::
[2025-06-05T17:33:59.160+0300] {sql.py:278} INFO - Executing: CREATE TABLE table_old (
    id INT,
    tecdocid TEXT, 
    carid TEXT,   
    ap2021 INT,
    ap2022 INT,
    type VARCHAR(10)
);

INSERT INTO table_old (id, tecdocid, carid, ap2021, ap2022, type) VALUES
(1, '1', '1', 20, 15, 'HCV'),
(2, '2', '1', 200, NULL, 'HCV'),
(3, '3', '1', NULL, 150, 'HCV'),
(4, '14', '1', 555, 445, 'HCV'),
(5, '5', NULL, 11, 10, 'PC'),
(6, '6', '2, 3', 55, 33, 'PC');

CREATE TABLE import_raw (
    tecdocid TEXT,
    carid TEXT,
    ap2023 INT,
    ap2024 INT,
    type VARCHAR(10)
);

INSERT INTO import_raw (tecdocid, carid, ap2023, ap2024, type) VALUES
('1', NULL, 10, 9, 'PC'),
('2,3', '1', 100, 89, 'HCV'),
(NULL, '2', 1, 0, 'PC'),
(NULL, '3', 22, NULL, 'PC'),
('14', '1', 444, 345, 'HCV'),
('2', '2', 40, 11, 'HCV');
[2025-06-05T17:33:59.176+0300] {base.py:84} INFO - Retrieving connection 'postgres_car'
[2025-06-05T17:33:59.189+0300] {logging_mixin.py:190} WARNING - /Users/margaritaborodina/ALL_PROJECTS/airflow_car/venv/lib/python3.10/site-packages/airflow/providers/postgres/hooks/postgres.py:30 RemovedInAirflow3Warning: This module is deprecated. Please use `airflow.providers.common.sql.hooks.sql`.
[2025-06-05T17:33:59.200+0300] {base.py:84} INFO - Retrieving connection 'postgres_car'
[2025-06-05T17:33:59.201+0300] {sql.py:218} WARNING - This setter is for backward compatibility and should not be used.
Since the introduction of connection property, the providers listed below breaks due to assigning value to self.connection in their __init__ method.
* apache-airflow-providers-mysql<5.7.1
* apache-airflow-providers-elasticsearch<5.5.1
* apache-airflow-providers-postgres<5.13.0
[2025-06-05T17:33:59.208+0300] {sql.py:553} INFO - Running statement: CREATE TABLE table_old (
    id INT,
    tecdocid TEXT, 
    carid TEXT,   
    ap2021 INT,
    ap2022 INT,
    type VARCHAR(10)
);

INSERT INTO table_old (id, tecdocid, carid, ap2021, ap2022, type) VALUES
(1, '1', '1', 20, 15, 'HCV'),
(2, '2', '1', 200, NULL, 'HCV'),
(3, '3', '1', NULL, 150, 'HCV'),
(4, '14', '1', 555, 445, 'HCV'),
(5, '5', NULL, 11, 10, 'PC'),
(6, '6', '2, 3', 55, 33, 'PC');

CREATE TABLE import_raw (
    tecdocid TEXT,
    carid TEXT,
    ap2023 INT,
    ap2024 INT,
    type VARCHAR(10)
);

INSERT INTO import_raw (tecdocid, carid, ap2023, ap2024, type) VALUES
('1', NULL, 10, 9, 'PC'),
('2,3', '1', 100, 89, 'HCV'),
(NULL, '2', 1, 0, 'PC'),
(NULL, '3', 22, NULL, 'PC'),
('14', '1', 444, 345, 'HCV'),
('2', '2', 40, 11, 'HCV');, parameters: None
[2025-06-05T17:33:59.231+0300] {sql.py:562} INFO - Rows affected: 6
[2025-06-05T17:33:59.247+0300] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-06-05T17:33:59.248+0300] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=push_autopark_import_pipeline, task_id=create_base_tables, run_id=manual__2025-06-05T14:33:48.957618+00:00, execution_date=20250605T143348, start_date=20250605T143358, end_date=20250605T143359
[2025-06-05T17:33:59.261+0300] {logging_mixin.py:190} INFO - Task instance in success state
[2025-06-05T17:33:59.262+0300] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2025-06-05T17:33:59.262+0300] {logging_mixin.py:190} INFO - Dag name:push_autopark_import_pipeline queued_at:2025-06-05 14:33:49.010775+00:00
[2025-06-05T17:33:59.263+0300] {logging_mixin.py:190} INFO - Task hostname:MacBook-Pro-Margarita.local operator:SQLExecuteQueryOperator
[2025-06-05T17:33:59.301+0300] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-06-05T17:33:59.347+0300] {taskinstance.py:3895} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-06-05T17:33:59.349+0300] {local_task_job_runner.py:245} INFO - ::endgroup::
